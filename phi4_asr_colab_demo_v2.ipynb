{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéôÔ∏è ASR + Tradu√ß√£o com **Microsoft Phi‚Äë4‚Äëmultimodal‚Äëinstruct** no Google Colab ‚Äî *Vers√£o Colab‚Äësafe*\n",
    "\n",
    "Esta vers√£o ajusta as depend√™ncias para evitar conflitos em ambientes Google Colab com GPU **Tesla T4** (Driver CUDA 12.x).\n",
    "- Remove **torchvision** (n√£o √© necess√°rio para √°udio) e fixa **Pillow** em `10.3.0` (evita conflito com Gradio).\n",
    "- N√£o tenta instalar **flash-attn** (T4 n√£o suporta); o notebook usa `attn_implementation='eager'`.\n",
    "\n",
    "Restante funcionalidade: igual √† vers√£o original (ASR PT/ES/EN, fallback de tradu√ß√£o para Bengali, Cloudflared).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Setup do ambiente (GPU, libs, FFmpeg, Cloudflared) ‚Äî Colab‚Äësafe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ac2bac",
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "# Verifica GPU dispon√≠vel\n",
    "!nvidia-smi || true\n",
    "\n",
    "# Instala depend√™ncias SEM upgrade (para n√£o sobrescrever Pillow depois)\n",
    "!pip install -q --no-warn-conflicts transformers==4.46.0 accelerate==1.3.0 soundfile==0.13.1 backoff==2.2.1 peft==0.13.2 gradio==4.44.1 jedi markupsafe==2.0.1\n",
    "\n",
    "# CR√çTICO: For√ßa Pillow 10.3.0 DEPOIS de todas as outras deps (evita erro 'is_directory' em PIL._util)\n",
    "!pip uninstall -y pillow Pillow PIL 2>/dev/null || true\n",
    "!pip install --force-reinstall --no-cache-dir --no-deps pillow==10.3.0\n",
    "\n",
    "# FFmpeg para processamento de √°udio\n",
    "!apt-get -qq update && apt-get -qq install -y ffmpeg >/dev/null 2>&1\n",
    "\n",
    "# Instala Cloudflared (TryCloudflare)\n",
    "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared && chmod +x cloudflared\n",
    "\n",
    "# Verifica vers√£o do Pillow instalada\n",
    "import PIL\n",
    "print(f\"‚úÖ Setup completo - Pillow {PIL.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce89e71",
   "metadata": {},
   "source": [
    "## 2) Login no Hugging Face com token\n",
    "Guarda o token em **Colab ‚Üí Secrets** com o nome `HF_TOKEN`. Se n√£o estiver presente, ser√° aberto um prompt para inserir manualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432e37f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "    if hf_token:\n",
    "        login(hf_token)\n",
    "        print('‚úÖ Login feito ao Hugging Face via Colab Secrets.')\n",
    "    else:\n",
    "        login()\n",
    "        print('‚úÖ Login feito ao Hugging Face via prompt.')\n",
    "except Exception as e:\n",
    "    print('Falha no login:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298f5a5c",
   "metadata": {},
   "source": [
    "## 3) Carregar o modelo **Phi‚Äë4‚Äëmultimodal‚Äëinstruct** e o Processor (usar aten√ß√£o *eager*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18873a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_USE_FLASH_ATTENTION_2\"] = \"0\"  # for√ßa desativa√ß√£o de FlashAttention2\n",
    "\n",
    "import torch, numpy as np, soundfile as sf\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor, AutoConfig, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import types\n",
    "import sys\n",
    "\n",
    "# Detecta GPU e configura dtype apropriado\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "USE_GPU = DEVICE == 'cuda'\n",
    "DTYPE = torch.float16 if USE_GPU else torch.float32\n",
    "print(f\"üîß Device: {DEVICE.upper()}, dtype: {DTYPE}\")\n",
    "\n",
    "# Verifica vers√µes compat√≠veis\n",
    "import PIL\n",
    "print(f\"üì¶ Pillow vers√£o: {PIL.__version__} (esperado: 10.3.0)\")\n",
    "if PIL.__version__ != \"10.3.0\":\n",
    "    print(\"‚ö†Ô∏è AVISO: Vers√£o de Pillow incompat√≠vel. Se encontrares erro 'is_directory', volta √† c√©lula 1 e executa novamente.\")\n",
    "\n",
    "# Configura SDPA apenas se tiver CUDA (CPU n√£o tem flash sdp)\n",
    "if USE_GPU:\n",
    "    try:\n",
    "        torch.backends.cuda.enable_flash_sdp(False)\n",
    "        torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "        torch.backends.cuda.enable_math_sdp(True)\n",
    "        print(\"üîß SDPA configurado: flash=False, mem_efficient=False, math=True\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "MODEL_ID = 'microsoft/Phi-4-multimodal-instruct'\n",
    "\n",
    "# Cache global para M2M tokenizer/model (evita reloads)\n",
    "_M2M_CACHE = {'tokenizer': None, 'model': None}\n",
    "\n",
    "def get_m2m_models():\n",
    "    \"\"\"Carrega M2M tokenizer/model uma vez e reutiliza.\"\"\"\n",
    "    if _M2M_CACHE['tokenizer'] is None:\n",
    "        m2m_id = 'facebook/m2m100_418M'\n",
    "        _M2M_CACHE['tokenizer'] = AutoTokenizer.from_pretrained(m2m_id)\n",
    "        _M2M_CACHE['model'] = AutoModelForSeq2SeqLM.from_pretrained(m2m_id).to(DEVICE)\n",
    "    return _M2M_CACHE['tokenizer'], _M2M_CACHE['model']\n",
    "\n",
    "# Load processor\n",
    "print(f\"Carregando processor para {MODEL_ID}...\") \n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "print(\"‚úÖ Processor carregado.\")\n",
    "\n",
    "# Carrega a configura√ß√£o e for√ßa attn_implementation='eager'\n",
    "config = AutoConfig.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "config.attn_implementation = 'eager'\n",
    "print(f\"üîß Configura√ß√£o de aten√ß√£o for√ßada para: {config.attn_implementation}\")\n",
    "\n",
    "# --- PATCH PRE-MODEL-LOAD: Injeta prepare_inputs_for_generation na classe antes de instanciar ---\n",
    "print(\"üîß Preparando patch de prepare_inputs_for_generation na classe Phi4MMModel...\")\n",
    "\n",
    "# Defini√ß√£o do m√©todo que vai ser adicionado\n",
    "def prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
    "    \"\"\"Prepara inputs para gera√ß√£o. Delega ao modelo interno se dispon√≠vel.\"\"\"\n",
    "    # Tenta delegar ao modelo interno (LlamaForCausalLM) se existir\n",
    "    if hasattr(self, 'model') and hasattr(self.model, 'prepare_inputs_for_generation'):\n",
    "        return self.model.prepare_inputs_for_generation(input_ids, **kwargs)\n",
    "    \n",
    "    # Se for um wrapper PEFT (LoraModel), tenta acessar a base_model\n",
    "    if hasattr(self, 'base_model') and hasattr(self.base_model, 'prepare_inputs_for_generation'):\n",
    "        return self.base_model.prepare_inputs_for_generation(input_ids, **kwargs)\n",
    "    \n",
    "    # Fallback: retorna um dict com os inputs principais\n",
    "    model_inputs = {\"input_ids\": input_ids}\n",
    "    for key in [\"attention_mask\", \"position_ids\", \"past_key_values\", \"use_cache\", \"inputs_embeds\",\n",
    "                \"pixel_values\", \"image_embeds\", \"image_attention_mask\"]:\n",
    "        if key in kwargs:\n",
    "            model_inputs[key] = kwargs[key]\n",
    "    return model_inputs\n",
    "\n",
    "# Monkey-patch a classe na cache de m√≥dulos ANTES do from_pretrained\n",
    "for module_name, module in list(sys.modules.items()):\n",
    "    if 'modeling_phi4mm' in module_name and hasattr(module, 'Phi4MMModel'):\n",
    "        print(f\"üîß Encontrado m√≥dulo {module_name}, adicionando prepare_inputs_for_generation √† classe...\")\n",
    "        if not hasattr(module.Phi4MMModel, 'prepare_inputs_for_generation'):\n",
    "            module.Phi4MMModel.prepare_inputs_for_generation = prepare_inputs_for_generation\n",
    "            print(f\"‚úÖ M√©todo adicionado √† classe Phi4MMModel em {module_name}\")\n",
    "        break\n",
    "\n",
    "# Carrega o modelo com eager attention, float16 para T4\n",
    "print(f\"Carregando modelo {MODEL_ID}... Isto pode levar alguns minutos.\")\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        config=config,\n",
    "        device_map='auto',\n",
    "        offload_folder=\"/tmp/\",\n",
    "        torch_dtype=DTYPE,\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation='eager',\n",
    "    )\n",
    "    print(\"‚úÖ Modelo carregado com sucesso.\")\n",
    "except Exception as e:\n",
    "    print(f\"üîß Detectado erro de prepare_inputs_for_generation durante load, reparando...\")\n",
    "    # Procura novamente e patcheia\n",
    "    for module_name, module in list(sys.modules.items()):\n",
    "        if 'modeling_phi4mm' in module_name and hasattr(module, 'Phi4MMModel'):\n",
    "            module.Phi4MMModel.prepare_inputs_for_generation = prepare_inputs_for_generation\n",
    "            print(f\"‚úÖ M√©todo adicionado √† classe (segunda tentativa)\")\n",
    "            break\n",
    "    # Tenta novamente\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        config=config,\n",
    "        device_map='auto',\n",
    "        offload_folder=\"/tmp/\",\n",
    "        torch_dtype=DTYPE,\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation='eager',\n",
    "    )\n",
    "\n",
    "# Garante que a configura√ß√£o interna fica em 'eager' e desativa cache\n",
    "try:\n",
    "    if hasattr(model, 'config'):\n",
    "        model.config.attn_implementation = 'eager'\n",
    "        model.config.use_cache = False\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Adapter de SPEECH (√°udio) - confirmado que existe no modelo\n",
    "print('üîß Carregando adapter \"speech-lora\"...')\n",
    "try:\n",
    "    model.load_adapter(MODEL_ID, adapter_name='speech', adapter_kwargs={'subfolder': 'speech-lora', 'offload_folder': '/tmp/'})\n",
    "    model.set_adapter('speech')\n",
    "    print('‚úÖ Adapter \"speech-lora\" carregado e ativado com sucesso.')\n",
    "except Exception as e:\n",
    "    print(f'‚ö†Ô∏è AVISO: speech-lora n√£o dispon√≠vel: {e}')\n",
    "    print('   Continua com modelo base (qualidade pode ser reduzida)')\n",
    "\n",
    "# Inje√ß√£o final: adiciona o m√©todo √† inst√¢ncia se ainda n√£o existir\n",
    "if not hasattr(model, 'prepare_inputs_for_generation'):\n",
    "    print(\"üîß Adicionando m√©todo √† inst√¢ncia do modelo...\")\n",
    "    model.prepare_inputs_for_generation = types.MethodType(prepare_inputs_for_generation, model)\n",
    "    print(\"‚úÖ M√©todo adicionado √† inst√¢ncia.\")\n",
    "else:\n",
    "    print(\"‚úÖ M√©todo j√° presente no modelo.\")\n",
    "\n",
    "# Garante que generation_config tamb√©m tem use_cache=False\n",
    "try:\n",
    "    if hasattr(model, 'generation_config'):\n",
    "        model.generation_config.use_cache = False\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(f\"\\n‚úÖ Modelo pronto para ASR! Device: {DEVICE.upper()}, dtype: {DTYPE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b016900",
   "metadata": {},
   "source": [
    "## 4) Fun√ß√µes de ASR e tradu√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5add4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import time, os, traceback\n",
    "\n",
    "SUPPORTED_SPEECH = {\n",
    "    'portugu√™s': 'Portuguese',\n",
    "    'espanhol': 'Spanish',\n",
    "    'ingl√™s': 'English',\n",
    "}\n",
    "\n",
    "\n",
    "def _safe_decode_text(processor, gen_ids):\n",
    "    try:\n",
    "        return processor.batch_decode(gen_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    except Exception:\n",
    "        # Fallback via tokenizer, se existir\n",
    "        try:\n",
    "            tok = getattr(processor, 'tokenizer', None)\n",
    "            if tok is not None:\n",
    "                return tok.batch_decode(gen_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        except Exception:\n",
    "            pass\n",
    "        # √öltimo recurso: tentar converter ids em string b√°sica\n",
    "        try:\n",
    "            return str(gen_ids)\n",
    "        except Exception:\n",
    "            return \"<decode_failed>\"\n",
    "\n",
    "\n",
    "def phi4_transcribe_or_translate(audio_path: str, in_lang_ui: str, out_lang_ui: str) -> str:\n",
    "    try:\n",
    "        t0 = time.perf_counter()\n",
    "        # Carrega √°udio\n",
    "        audio_array, sr = sf.read(audio_path)\n",
    "        # Converte para mono se necess√°rio\n",
    "        if audio_array.ndim > 1:\n",
    "            audio_array = audio_array.mean(axis=1)\n",
    "        # Limita dura√ß√£o para evitar demora excessiva\n",
    "        max_seconds = 30\n",
    "        max_samples = int(sr * max_seconds)\n",
    "        if audio_array.shape[0] > max_samples:\n",
    "            audio_array = audio_array[:max_samples]\n",
    "        audio_array = np.asarray(audio_array, dtype=np.float32)\n",
    "\n",
    "        # Instru√ß√£o base (sem placeholder de √°udio)\n",
    "        if out_lang_ui.lower() == \"bengali\":\n",
    "            base_instruction = f\"Transcribe the audio clip into text in {SUPPORTED_SPEECH.get(in_lang_ui, 'Portuguese')}.\"\n",
    "        else:\n",
    "            if in_lang_ui == out_lang_ui:\n",
    "                base_instruction = f\"Transcribe the audio clip into text in {SUPPORTED_SPEECH.get(in_lang_ui, 'Portuguese')}.\"\n",
    "            else:\n",
    "                base_instruction = (\n",
    "                    f\"Transcribe the audio to text in {SUPPORTED_SPEECH.get(in_lang_ui, 'Portuguese')}, \"\n",
    "                    f\"and then translate the audio to {out_lang_ui}. Use <sep> as a separator between the original transcript and the translation.\"\n",
    "                )\n",
    "\n",
    "        # O Processor do Phi-4 precisa de tokens de √°udio no texto.\n",
    "        # Tentamos com tokens comuns suportados: \"<|audio|>\" e \"<|audio_1|>\".\n",
    "        AUDIO_PLACEHOLDERS = [\"<|audio|>\", \"<|audio_1|>\"]\n",
    "\n",
    "        proc_inputs = None\n",
    "        errors = []\n",
    "\n",
    "        # Usa sempre o array (waveform) + sr; evita caminhos para reduzir ambiguidade\n",
    "        for ph in AUDIO_PLACEHOLDERS:\n",
    "            task_prompt = f\"{ph}\\n{base_instruction}\"\n",
    "            try:\n",
    "                params = {\"text\": [task_prompt], \"audios\": [(audio_array, sr)]}\n",
    "                proc_inputs = processor(return_tensors='pt', **params).to(DEVICE)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                errors.append(f\"placeholder={ph}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if proc_inputs is None:\n",
    "            return (\n",
    "                \"‚ö†Ô∏è Falha ao preparar inputs multimodais para gera√ß√£o.\\n\" +\n",
    "                \"\\n\".join(errors[:5])\n",
    "            )\n",
    "\n",
    "        # Gera√ß√£o mais r√°pida/est√°vel\n",
    "        try:\n",
    "            model.eval()\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            with torch.inference_mode():\n",
    "                gen_ids = model.generate(\n",
    "                    **proc_inputs,\n",
    "                    max_new_tokens=256,\n",
    "                    do_sample=False,\n",
    "                    temperature=0.0,\n",
    "                    top_p=1.0,\n",
    "                    num_beams=1,\n",
    "                    early_stopping=True,\n",
    "                    use_cache=False  # Evita DynamicCache.get_usable_length errors\n",
    "                )\n",
    "        except Exception as e:\n",
    "            tb = traceback.format_exc(limit=2)\n",
    "            return f\"‚ö†Ô∏è Erro durante gera√ß√£o: {e}\\n{tb}\\nTenta um √°udio mais curto (<= {max_seconds}s).\"\n",
    "\n",
    "        # Se existir input_ids, corta o prefixo do prompt\n",
    "        try:\n",
    "            cut = proc_inputs['input_ids'].shape[1]\n",
    "            gen_ids = gen_ids[:, cut:]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        text = _safe_decode_text(processor, gen_ids)\n",
    "\n",
    "        # Se n√£o for bengali, tenta extrair ap√≥s <sep>\n",
    "        if out_lang_ui.lower() != 'bengali':\n",
    "            result = text.split('<sep>')[-1].strip() if '<sep>' in text else text\n",
    "            return result\n",
    "\n",
    "        # Tradu√ß√£o fallback para bengali (usa cache global)\n",
    "        try:\n",
    "            tok, m = get_m2m_models()\n",
    "            lang_code_map = {'portugu√™s': 'pt', 'espanhol': 'es', 'ingl√™s': 'en'}\n",
    "            src_code = lang_code_map.get(in_lang_ui, 'pt')\n",
    "            tok.src_lang = src_code\n",
    "            inputs_tx = tok(text, return_tensors='pt').to(DEVICE)\n",
    "            out_ids = m.generate(**inputs_tx, forced_bos_token_id=tok.get_lang_id('bn'))\n",
    "            translated = tok.batch_decode(out_ids, skip_special_tokens=True)[0]\n",
    "            return translated\n",
    "        except Exception as e:\n",
    "            return f\"(Transcri√ß√£o)\\n{text}\\n\\n‚ö†Ô∏è Fallback de tradu√ß√£o para bengali falhou: {e}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        # Captura qualquer erro n√£o tratado e retorna texto detalhado para a UI\n",
    "        return \"‚ö†Ô∏è Erro inesperado na pipeline ASR:\\n\" + str(e) + \"\\n\\n\" + traceback.format_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Interface Gradio e publica√ß√£o via Cloudflared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# WORKAROUND: Monkey-patch gradio_client para corrigir bug de schema parsing\n",
    "try:\n",
    "    from gradio_client import utils as gc_utils\n",
    "    original_get_type = gc_utils.get_type if hasattr(gc_utils, 'get_type') else None\n",
    "    \n",
    "    if original_get_type:\n",
    "        def patched_get_type(schema):\n",
    "            \"\"\"Wrapper que lida com schemas booleanos.\"\"\"\n",
    "            # Se schema for bool (True/False), retorna tipo gen√©rico\n",
    "            if isinstance(schema, bool):\n",
    "                return \"any\"\n",
    "            # Sen√£o, chama fun√ß√£o original\n",
    "            return original_get_type(schema)\n",
    "        \n",
    "        gc_utils.get_type = patched_get_type\n",
    "        print(\"‚úÖ Patch aplicado ao gradio_client.utils.get_type\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è N√£o foi poss√≠vel aplicar patch: {e}\")\n",
    "\n",
    "INPUT_LANGS = ['portugu√™s', 'espanhol', 'ingl√™s']\n",
    "OUTPUT_LANGS = ['portugu√™s', 'espanhol', 'ingl√™s', 'bengali']\n",
    "CF_PUBLIC_URL = None\n",
    "\n",
    "\n",
    "def run_pipeline(audio_path, in_lang_ui, out_lang_ui):\n",
    "    try:\n",
    "        if not audio_path:\n",
    "            return 'Grava ou faz upload de um √°udio primeiro üòâ'\n",
    "        return phi4_transcribe_or_translate(audio_path, in_lang_ui, out_lang_ui)\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        return \"‚ö†Ô∏è Erro ao executar a pipeline:\\n\" + str(e) + \"\\n\\n\" + traceback.format_exc()\n",
    "\n",
    "\n",
    "def get_public_url():\n",
    "    global CF_PUBLIC_URL\n",
    "    return CF_PUBLIC_URL or 'URL ainda n√£o dispon√≠vel: lan√ßa o t√∫nel Cloudflared na c√©lula seguinte.'\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown('## üéôÔ∏è ASR + Tradu√ß√£o com **Phi-4-multimodal-instruct**')\n",
    "    gr.Markdown('**Fala no microfone OU faz upload de um ficheiro de √°udio** (MP3, WAV, etc.) e recebe o texto na l√≠ngua selecionada.')\n",
    "    gr.Markdown('üìÅ Nota: sem microfone no Colab, usa upload de ficheiro. Bengali √© tradu√ß√£o de fallback ap√≥s transcri√ß√£o.')\n",
    "\n",
    "    with gr.Row():\n",
    "        in_lang = gr.Dropdown(choices=INPUT_LANGS, value='portugu√™s', label='L√≠ngua de entrada (fala)')\n",
    "        out_lang = gr.Dropdown(choices=OUTPUT_LANGS, value='portugu√™s', label='L√≠ngua de sa√≠da (texto)')\n",
    "\n",
    "    audio_input = gr.Audio(\n",
    "        sources=['microphone', 'upload'],\n",
    "        type='filepath',\n",
    "        label='üéôÔ∏è Microfone ou Upload de √Åudio (MP3/WAV)'\n",
    "    )\n",
    "\n",
    "    out_text = gr.Textbox(label='Texto de sa√≠da', interactive=False, lines=8)\n",
    "    public_url_box = gr.Textbox(label='Public URL (Cloudflared)', interactive=False)\n",
    "\n",
    "    btn = gr.Button('üöÄ Transcrever / Traduzir')\n",
    "    btn.click(run_pipeline, inputs=[audio_input, in_lang, out_lang], outputs=out_text)\n",
    "\n",
    "    btn_show_url = gr.Button('üîó Mostrar Public URL')\n",
    "    btn_show_url.click(get_public_url, inputs=None, outputs=public_url_box)\n",
    "\n",
    "    # Dica r√°pida na UI sobre limites e erros\n",
    "    gr.Markdown('‚ÑπÔ∏è Dica: usa ficheiros ‚â§ 30s. Erros completos aparecem nesta caixa de texto.')\n",
    "\n",
    "demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Lan√ßar a interface e expor com Cloudflared (TryCloudflare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469ae4a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d27aaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, re, time, shutil, os, signal, socket\n",
    "\n",
    "# Fecha inst√¢ncia anterior para evitar relaunch e erros de SSE\n",
    "try:\n",
    "    demo.close()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Escolhe uma porta livre dinamicamente\n",
    "\n",
    "def is_port_free(port: int) -> bool:\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
    "        return s.connect_ex((\"127.0.0.1\", port)) != 0\n",
    "\n",
    "\n",
    "def get_free_port(preferred: int | None = None) -> int:\n",
    "    # Tenta usar porta do env se estiver livre\n",
    "    if preferred and is_port_free(preferred):\n",
    "        return preferred\n",
    "    # Procura uma porta livre (ephemeral)\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "        s.bind((\"\", 0))\n",
    "        return s.getsockname()[1]\n",
    "\n",
    "env_port = None\n",
    "try:\n",
    "    env_port = int(os.environ.get(\"GRADIO_SERVER_PORT\", \"\"))\n",
    "except Exception:\n",
    "    env_port = None\n",
    "\n",
    "PORT = get_free_port(env_port or 7866)\n",
    "os.environ[\"GRADIO_SERVER_PORT\"] = str(PORT)\n",
    "print(f\"üîå A usar porta livre: {PORT}\")\n",
    "\n",
    "# Lan√ßa Gradio sem queue; desativa share interno (usamos Cloudflared)\n",
    "try:\n",
    "    app = demo.launch(\n",
    "        server_name='0.0.0.0',\n",
    "        server_port=PORT,\n",
    "        share=False,\n",
    "        inbrowser=False,\n",
    "        show_error=True,\n",
    "        prevent_thread_lock=True,\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(\"‚ö†Ô∏è Localhost indispon√≠vel para acesso direto. A criar link share do Gradio...\")\n",
    "    print(f\"Detalhe: {e}\")\n",
    "    app = demo.launch(\n",
    "        server_name='0.0.0.0',\n",
    "        server_port=PORT,\n",
    "        share=True,\n",
    "        inbrowser=False,\n",
    "        show_error=True,\n",
    "        prevent_thread_lock=True,\n",
    "    )\n",
    "\n",
    "print(f\"üåê UI local: http://localhost:{PORT}\")\n",
    "\n",
    "# Pequeno atraso para o servidor arrancar\n",
    "time.sleep(2)\n",
    "\n",
    "# Termina t√∫nel anterior se existir\n",
    "if 'CF_PROC' in globals():\n",
    "    try:\n",
    "        if CF_PROC and CF_PROC.poll() is None:\n",
    "            CF_PROC.terminate()\n",
    "            try:\n",
    "                CF_PROC.wait(timeout=5)\n",
    "            except subprocess.TimeoutExpired:\n",
    "                CF_PROC.kill()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Detecta bin√°rio do cloudflared (PATH ou ficheiro local ./cloudflared)\n",
    "\n",
    "def ensure_cloudflared_path():\n",
    "    path = shutil.which('cloudflared')\n",
    "    if path:\n",
    "        return path\n",
    "    local = './cloudflared'\n",
    "    if os.path.exists(local) and os.access(local, os.X_OK):\n",
    "        return local\n",
    "    return None\n",
    "\n",
    "cf_bin = ensure_cloudflared_path()\n",
    "if not cf_bin:\n",
    "    print(\"‚ö†Ô∏è Cloudflared n√£o encontrado. Usa a c√©lula de setup para baixar (wget ‚Ä¶) ou instala manualmente.\")\n",
    "else:\n",
    "    # Inicia t√∫nel para o servidor local\n",
    "    CF_PROC = subprocess.Popen(\n",
    "        [cf_bin, 'tunnel', '--url', f'http://localhost:{PORT}'],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        text=True,\n",
    "        bufsize=1,\n",
    "        universal_newlines=True,\n",
    "    )\n",
    "\n",
    "    url_pattern = re.compile(r\"(https://[a-z0-9-]+\\.trycloudflare\\.com)\")\n",
    "    public_url = None\n",
    "    start = time.time()\n",
    "\n",
    "    try:\n",
    "        # L√™ linhas do cloudflared at√© extrair o URL (timeout de 30s)\n",
    "        while True:\n",
    "            if CF_PROC.poll() is not None:\n",
    "                break\n",
    "            ln = CF_PROC.stdout.readline()\n",
    "            if not ln:\n",
    "                if time.time() - start > 30:\n",
    "                    break\n",
    "                time.sleep(0.1)\n",
    "                continue\n",
    "            m = url_pattern.search(ln)\n",
    "            if m:\n",
    "                public_url = m.group(1)\n",
    "                # Atualiza vari√°vel global usada pelo bot√£o \"Mostrar Public URL\"\n",
    "                try:\n",
    "                    CF_PUBLIC_URL\n",
    "                except NameError:\n",
    "                    CF_PUBLIC_URL = None\n",
    "                CF_PUBLIC_URL = public_url\n",
    "                print(f\"‚úÖ Public URL: {public_url}\\n\")\n",
    "                print(\"Dica: usa o bot√£o 'Mostrar Public URL' na UI para copiar.\")\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Erro a ler sa√≠da do cloudflared:\", e)\n",
    "\n",
    "    if not public_url:\n",
    "        print('üìÅ Sem URL p√∫blico. Verifica conectividade de rede do runtime.')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
